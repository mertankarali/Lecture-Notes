%
\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% ADD PACKAGES here:
%

\usepackage{amsmath,amsfonts,graphicx}

%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf EE302 - Feedback Systems
	\hfill Spring 2019} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1 \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #2 \hfill } }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1}{Lecture #1}

   \vspace*{4mm}
}
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
u         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\begin{document}

% Lecture Details
\lecture{20}{Asst. Prof. M. Mert Ankarali}

\par

\section{State-Space Analysis of LTI Systems}

\subsection{State-Space to TF}

Let's first re-visit the conversion from a state-space representation to 
the transfer function representations for LTI systems. 

Note that a SS representation of an $n^{th}$ order LTI system has the from below.
%
\begin{align*}
  \mathrm{Let} \ x(t) &\in \mathbb{R}^n \ , \ y(t) \in \mathbb{R} \ ,\  u(t) \in
  \mathbb{R} , \\
  \dot{x}(t) &= A x(t) + B u(t) , \\
  y(t) &= C x(t) + D u(t) , \\
  \mathrm{where} \ A &\in \mathbb{R}^{n \times n} \ , \ 
    B \in \mathbb{R}^{n \times 1} \ ,\  C \in \mathbb{R}^{1 \times n} \ , \ D \in \mathbb{R}
\end{align*}
%
In order to convert state-space to transfer function, we start with taking the Laplace transform of the 
both sides of the state-equation 
%
\begin{align*}
\dot{x}(t) &= A x(t) + B u(t) 
\\
s X(s) &= A X(s) + B U(s)
\\
s X(s) - A X(s) &=  B U(s)
\\
\left( s I - A) X(s) \right) &=  B U(s)
\\
X(s) &= \left( s I - A \right)^{-1} B U(s)
\end{align*}
%
Now let's concentrate on the output equation
%
\begin{align*}
y(t) &= C x(t) + D u(t)
\\
Y(s) &= \left[ C \left( s I - A \right)^{-1} B + D \right] U(s)
\\
G(s) &= C \left( s I - A \right)^{-1} B + D
\end{align*}

\vspace{12pt}

\textbf{Example:} Let $p$ be a pole of $G(s)$, then show that
$p$ is also an eigenvalue of $A$. 

\textbf{Solution:} Let 
%
\begin{align*}
  G(s) = \frac{n(s)}{d(s)}
\end{align*}
%
If $p$ is a pole of $G(s)$, then $d(s)|_{p} = 0$. Now let's analyze
the dependence of $G(s)$ to the state-space form. 
%
\begin{align*}
  G(s) &= \left[ C \left(s I - A \right)^{-1} B + D \right]
\\
  \left(s I - G \right)^{-1} &= 
\frac{\mathrm{Adj} \left(s I - A \right) }{\mathrm{det} \left(s I - A
                               \right) }
\\
 G(s) &= \frac{C \mathrm{Adj} \left(s I - G \right) B + D \mathrm{det}
        \left(s I - A
                               \right)}{\mathrm{det} \left(s I - A
                               \right) }
\end{align*}
%
If $p$ is a pole of $G(s)$, then 
%
\begin{align*}
\mathrm{det} \left(s I - A \right)|_{s = p} = 0
\end{align*}
%
Obviously $p$ is an eigenvalue of $A$.


\subsection{Similarity Transformations}

Now let's define a new ``state-vector'' $\hat{x}$ such that
%
\begin{align*}
  P x(t) = \hat{x}(t) \quad \mathrm{where}
\\
  P \in \mathbb{R}^{n \times n} \quad , \mathrm{det}(P) \neq 0
\end{align*}
%
Then we can transform the state-space equations using $P$ as
%
\begin{align*}
  P^{-1} \dot{\hat{x}}(t) = A P^{-1} \hat{x}(t) + B u(t) 
\quad &, \quad y(t) = C P^{-1} \hat{x}(t) + D u(t)
\\
  \dot{\hat{x}} = P A P^{-1} \hat{x}(t)+ P B u(t)
\quad &, \quad y(t) = C P^{-1} \hat{x}(t) + D u(t)
\end{align*}
%
The ``new'' state-space representation is obtained as
%
\begin{align*}
  \dot{\hat{x}}(t)  &= \hat{A} \hat{x}(t) + \hat{B} u(t)
    \\
  y(t) &= \hat{C} x(t) + \hat{D} u(t)
\\
\hat{A} &= P  A P^{-1} \ , \ \hat{B} = P  B \ , \ \hat{C} = C P^{-1} \ ,
  \ \hat{D} = D
\end{align*}
%
Since there exist infinitely many non-singular $n \times n$
matrices, for a given LTI system, there exist infinitely 
many different but equivalent state-space representations.

\vspace{6pt}

\textbf{Example:} Show that $A \in \mathbb{R}^{n \times n}$ and $P^{-1} A
P$, where $P \in \mathbb{R}^{n \times n}$ and $\mathrm{det}(P) \neq 0$, have the
same characteristic equation

\textbf{Solution:} 
\begin{align*}
  \mathrm{det}\left(\lambda I - P^{-1} A P \right) &= 
\mathrm{det}\left( \lambda P^{-1} I P - P^{-1} A P \right)
\\
&= \mathrm{det} \left( P^{-1} \left( \lambda I - A \right) P \right)
\\
&= \mathrm{det} \left( P^{-1} \right)
\mathrm{det} \left( \lambda I - A \right) 
\mathrm{det} \left( P \right)
\\
&= \mathrm{det} \left( P^{-1} \right) \mathrm{det} \left( P \right)
\mathrm{det} \left( \lambda I - A \right) 
\\
\mathrm{det}\left(\lambda I - P^{-1} A P \right) &= \mathrm{det} \left( \lambda I - A \right) 
\end{align*}

\subsubsection{Invariance of Transfer Functions Under Similarity
  Transformation}

Consider the two different state-space representations
%
\begin{align*}
  \dot{x}(t) = A x(t) + B u(t)  \quad & \quad \quad \dot{\hat{x}}(t) = \hat{A} \hat{x}(t) + \hat{B} u(t) 
\\
  y(t) = C x(t) + D u(t) \quad & \quad \quad  y(t) = \hat{C} \hat{x}(t) + \hat{D} u(t)
\end{align*}
%
where they are related with the following similarity transformation
%
\begin{align*}
P x(t) = \hat{x}(t) \ , \
\hat{G} &= P  A P^{-1} \ , \ \hat{B} = P  B \ , \ \hat{C} = C P^{-1} \ ,
  \ \hat{D} = D
\end{align*}
%
Let's compute the transfer function for the second representation
%
\begin{align*}
  \hat{G}(s) &= \left[ \hat{C} \left(s I - \hat{A} \right)^{-1} \hat{B}
  + \hat{D} \right]
\\
&= \left[ C P^{-1} \left(s I - P  A P^{-1} \right)^{-1} P B
  + D \right]
\\
&= \left[ C P^{-1} \left( P \left( s I -   A \right) P^{-1} \right)^{-1} P B
  + D \right]
\\
&= \left[ C P^{-1} P \left( s I -   A \right)^{-1} P^{-1} P B
  + D \right]
\\
&= \left[ C \left( s I -   A \right)^{-1} B + D \right]
\\
\hat{G}(s) &= G(s)
\end{align*}


\subsection{Canonical State-Space Realizations}

We know  that for a given LTI system, there exist infinitely many 
different SS representations. We previously learnt some methods 
to convert  a TF/ODE into State-Space form. We will now re-visit
them and talk about the canonical state-space forms.

For the sake of clarity, derivations are given for a general $3^{rd}$ order LTI system.

\subsection{Controllable Canonical Form}

In this method of realization, we use the fact the system is
LTI. Let's consider the transfer function of the system and let's 
perform some LTI operations.
%
\begin{align*}
Y(s) &= \frac{ b_3 s^3 + b_2 s^2  + b_1 s + b_0 }{ s^3 + a_2 s^2 + a_1 s + a_0} U(s)
\\
&= \left( b_3 s^3 + b_2 s^2  + b_1 s + b_0 \right)  
\frac{1}{ s^3 + a_2 s^2 + a_1 s + a_0 } U(s) 
\\
&= G_2(s) G_1(s) U(s) \ \mathrm{where} 
\\
G_1(s) &= \frac{H(s)}{U(s)} = \frac{1}{ s^3 + a_2 s^2 + a_1 s + a_0 } 
\\
G_2(s) &= \frac{Y(z)}{H(z)} = b_3 s^3 + b_2 s^2  + b_1 s + b_0
\end{align*}
%
As you can see we introduced an intermediate variable $h(t)$ or with a
Laplace transform of $H(s)$. First transfer function has static input
dynamics, operates on $u(t)$, and produces an output, i.e. $h(t)$. 
Second transfer function is a ``non-causal'' system and operates on $h(t)$ and produces output
$y(t)$. If we write the ODEs of both systems we obtain
%
\begin{align*}
\dddot{h} &= -a_2 \ddot{h} - a_1 \dot{h} - a_0 h + u
\\
y &= b_3 \dddot{h} + b_2 \ddot{h} + b_1 \dot{h} + b_0 h
\end{align*}
%
Now let the state-variables be 
$x = \left[ \begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array} \right] 
= \left[ \begin{array}{c} h \\ \dot{h} \\ \ddot{h} \end{array} \right]$. Then,
individual state equations take the form
%
\begin{align*}
\dot{x_1} &= x_2
\\
\dot{x_2} &= x_3
\\
\dot{x_3} &= -a_2 x_3 - a_1 x_2 - a_0 x_1 + u
\end{align*}
%
and the output equation take the form
%
\begin{align*}
y &= b_3\left( -a_2 x_3 - a_1 x_2 - a_0 x_1 + u \right) + b_2 x_3 +
    b_1 x_2 + b_0 x_1
\\
&= ( b_0 - b_3 a_0 ) x_1 + ( b_1 - b_3 a_1 ) x_2 + ( b_2 - b_3 a_2 ) x_3 + b_3 u
\end{align*}
%
If we re-write the equations in matrix form we obtain the state-space representation as
%
\begin{align*}
	\dot{x} &= \left[ \begin{array}{ccc} 0 & 1 & 0  \\  0 & 0 & 1 \\  -a_0 & -a_1 & -a_2 \end{array} \right] x 
	+ \left[ \begin{array}{c} 0 \\ 0 \\ 1 \end{array} \right] u
	\\
	y &= \left[ \begin{array}{ccc} ( b_0 - b_3 a_0 ) & ( b_1 - b_3 a_1 )  & ( b_2 - b_3 a_2 ) \end{array} \right] x
	+ \left[ b_3 \right] u
\end{align*}

If we obtain a state-space model from this approach, the form
will be in \textit{controllable canonical form}. 

For a general $n^{th}$ order transfer function controllable
canonical form has the following $A \ ,  B \ ,  C \ , \& \ D$
matrices

\begin{align*}
A &= \left[ \begin{array}{ccccc} 0 & 1 & 0 & \cdots & 0 \\ 0 & 0 & 1 &
                                                                      \cdots & 0
\\ \vdots & \vdots & \vdots & & \vdots
\\ 0 & 0 & 0 & \cdots & 1
    \\ -a_0 & -a_1 & -a_2 & \cdots & -a_{n-1} \end{array} \right]
\quad , \quad 
B = \left[ \begin{array}{c} 0\\ 0 \\ \vdots \\ 0
    \\ 1 \end{array} \right]
\\ C &= \left[ \begin{array}{cccc} (b_0 - b_n a_0) 
  &  (b_1 - b_n a_1) & \cdots & (b_{n-1} - b_n a_{n-1}) \end{array} \right]
\quad , \quad
D = b_n
\end{align*}

\subsection{Observable Canonical Form}

In this method will obtain a different minimal state-space realization,
the form is called observable canonical form.
The process is different and state-space structure will have a
different topology. Let's start with a $3^{rd}$ transfer function 
and perform some grouping based on the $s$ elements.
%
\begin{align*}
&Y(s) = \frac{ b_3 s^3 + b_2 s^2  + b_1 s + b_0 }{ s^3 + a_2 s^2 + a_1 s + a_0} U(s)
\\
&Y(s) \left( s^3 + a_2 s^2 + a_1 s + a_0 \right) = \left( b_3 s^3 + b_2 s^2  + b_1 s + b_0 \right) U(s)
\\
&s^3 Y(s) = b_3 s^3 U(s) + s^2 \left( -a_2 Y(s) + b_2 U(s) \right) + s \left( -a_1 Y(s) + b_1 U(s) \right) + 
\left( -a_0 Y(s) + b_0 U(s) \right) 
\end{align*}
%
Let's multiply both sides with $\frac{1}{s^3}$ and perform further grouping
%
\begin{align*}
&Y(s) = b_3 U(s) + \frac{1}{s} \left( -a_2 Y(s) + b_2 U(s) \right) + \frac{1}{s^2}  \left( -a_1 Y(s) + b_1 U(s) \right) + 
\frac{1}{s^3} \left( -a_0 Y(s) + b_0 U(s) \right) 
\\
&Y(s) = b_3 U(s) + \frac{1}{s} \left[ \left( -a_2 Y(s) + b_2 U(s) \right) + \frac{1}{s}  \left\lbrace \left( -a_1 Y(s) + b_1 U(s) \right) + 
\frac{1}{s} \left( -a_0 Y(s) + b_0 U(s) \right) \right\rbrace \right]
\end{align*}
%
Let the Laplace domain representations of state variables $X(s) = \left[ \begin{array}{c} X_1(s) \\ X_2(s) \\ X_3(s) \end{array} \right]$ defined as 
%
\begin{align*}
X_1(s) &= \frac{1}{s} \left( -a_0 Y(s) + b_0 U(s) \right)
\\
X_2(s) &= \frac{1}{s}  \left\lbrace \left( -a_1 Y(s) + b_1 U(s) \right) + 
\frac{1}{s} \left( -a_0 Y(s) + b_0 U(s) \right) \right\rbrace
\\
X_3(s) &= \frac{1}{s} \left[ \left( -a_2 Y(s) + b_2 U(s) \right) + \frac{1}{s}  \left\lbrace \left( -a_1 Y(s) + b_1 U(s) \right) + 
\frac{1}{s} \left( -a_0 Y(s) + b_0 U(s) \right) \right\rbrace \right]
\end{align*}
%
In this context output equation in $s$ and time domains simply takes the form
%
\begin{align*}
	Y(s) = X_3(s) + b_3 U(S) \quad \rightarrow \quad y(t) = x_3(t) + b_3 u(t)
\end{align*}
% 
Dependently the state equations (in $s$ and time domains) take the form
%
%
\begin{align*}
s X_1(s) = -a_0 X_3(s)  + ( b_0 - a_0 b_3) U(s) \quad &\rightarrow \quad \dot{x}_1 = -a_0 x_3  + ( b_0 - a_0 b_3) u
\\
s X_2(s) = X_1(s)  -a_1 X_3(s) + ( b_1 - a_1 b_3 ) U(s)  \quad &\rightarrow \quad \dot{x}_2 = x_1  - a_1 x_3 + ( b_1 - a_1 b_3 ) u
\\
s X_3(s) = X_2(s)  -a_2 X_3(s) + ( b_2 - a_2 b_3 ) U(s)  \quad &\rightarrow \quad \dot{x}_3 = x_2  - a_2 x_3 + ( b_2 - a_2 b_3 ) u
\end{align*}
%
If we re-write all equations in matrix form, we obtain the state-space representation as
%
\begin{align*}
	\dot{x} &= \left[ \begin{array}{ccc} 0 & 0 & -a_0  \\  1 & 0 & -a_1 \\  0 & 1 & -a_2 \end{array} \right] x 
	+ \left[ \begin{array}{c} ( b_0 - b_3 a_0 ) \\ ( b_1 - b_3 a_1 ) \\ ( b_2 - b_3 a_2 ) \end{array} \right] u
	\\
	y &= \left[ \begin{array}{ccc} 0 & 0  & 1 \end{array} \right] x
	+ \left[ b_3 \right] u
\end{align*}

If we obtain a state-space model from this method, the form
will be in \textit{observable canonical form}. Thus we can call this representation also as 
\textit{observable canonical realization}. This form and
representation is the dual of the previous representation. 

For a general $n^{th}$ order system controllable
canonical form has the following $A \ ,  B \ ,  C \ , \& \ D$
matrices

\begin{align*}
A &= \left[ \begin{array}{ccccc} 0 & 0 & \cdots & 0 & -a_0 
              \\ 1 & 0 & \cdots & 0 & -a_1 
\\ \vdots & \vdots & \vdots & \vdots & \vdots
\\ 0 & 0 & \cdots & 0 & -a_{n-2}
    \\ 0 & 0 & \cdots & 1 & -a_{n-1} \end{array} \right]
\quad , \quad 
B = \left[ \begin{array}{c} (b_0 - b_n a_0)  \\ (b_1 - b_n
             a_1 ) \\ \vdots \\ (b_{n-2} - b_n a_{n-2} ) \\   (b_{n-1} - b_n
             a_{n-1}) 
\end{array} \right]
\\ C &= \left[ \begin{array}{ccccc} 0 & 0 & \cdots &  0 & 1 \end{array} \right]
\quad , \quad
D = b_n
\end{align*}

\subsubsection*{Diagonal Canonical Form}

If the transfer function of the LTI system 
has distinct poles, we can expand it 
using partial fraction expansion 
%
\begin{align*}
Y(s) &= \left[ b_0 + \frac{c_1}{s - p_1} + \frac{c_2}{s - p_2} 
+ \frac{c_3}{s - p_3} \right] U(s)
\end{align*}
%
Now let's concentrate on the candidate ``state variables''
and try to write state evaluation equations
%
\begin{align*}
X_1(s) &= \frac{1}{s - p_1} U(s) \quad \rightarrow \quad \dot{x}_1 =  p_1 x_1+ u
\\
X_2(s) &= \frac{1}{s - p_2} U(s) \quad \rightarrow \quad \dot{x}_2 = p_2 x_2+ u
\\
X_3(s) &= \frac{1}{s - p_3} U(s) \quad \rightarrow \quad \dot{x}_3 = p_3 x_3 + u
\end{align*}
%
where as output equation can be derived as
%
\begin{align*}
y(t) &= b_3 u(t) + c_1 x_1(t) + c_2 x_2(t) + c_3 x_3(t)
\end{align*}
%
If we combine the state and output equations, we
can obtain the state space form as
%
%
\begin{align*}
  \dot{\mathbf{x}}(t) &= \left[ \begin{array}{ccc} p_1 & 0 & 0\\ 0 & p_2 & 0
    \\ 0 & 0 & p_3 \end{array} \right] \mathbf{x}(t)
   + 
  \left[ \begin{array}{c} 1 \\ 1
    \\ 1 \end{array} \right] u(t)
\\
y(t) &= \left[ \begin{array}{ccc} c_1 & c_2 & c_3 \end{array} \right] \mathbf{x}(t)
+ b_3 u(t)
\end{align*}
%
where 
%
\begin{align*}
\mathbf{x} = \left[ \begin{array}{c} x_1(t) \\ x_2(t) \\
x_3(t) \end{array} \right] \quad , \quad
A = \left[ \begin{array}{ccc} p_1 & 0 & 0 \\ 0 & p_2 & 0
    \\ 0 & 0 & p_3 \end{array} \right]
\quad , \quad 
B = \left[ \begin{array}{c} 1 \\ 1
    \\ 1 \end{array} \right]
\quad , \quad
C = \left[ \begin{array}{ccc} c_1 & c_2 & c_3 \end{array} \right]
\quad , \quad
D = b_3
\end{align*}
%

The form obtained with this approach is called
diagonal canonical form. Obviously, this form is
not applicable for ``some'' systems that has repeated roots.

For a general $n^{th}$ order system with distinct
roots diagonal canonical form has the following 
$A \ ,  B \ ,  C \ , \& \ D$ matrices
%
\begin{align*}
A &= \left[ \begin{array}{ccccc} p_1 & 0 & \cdots & 0 & 0
              \\ 0 & p_2 & \cdots & 0 & 0
\\ \vdots & \vdots & \vdots & \vdots & \vdots
\\ 0 & 0 & \cdots & p_{n-1} & 0
    \\ 0 & 0 & \cdots & 0 & p_n \end{array} \right]
\quad , \quad 
B = \left[ \begin{array}{c} 1 \\ 1 \\ \vdots \\ 1 \\  1
\end{array} \right]
\\ C &= \left[ \begin{array}{ccccc} c_1 & c_2 & \cdots &  c_{n-1} & c_n \end{array} \right]
\quad , \quad
D = b_n
\end{align*}

\newpage

\textbf{Example:} Given that
%
\begin{align*}
G(s) &= \frac{s^2 + 8 s + 10}{s^2 + 3 s + 2}
\end{align*}
%
find a controllable, observable, and  diagonal canonical state-space representation of the given TF.

\vspace{6pt}

\textbf{Solution:}

If we follow the derivation of controllable canonical form for a
second order system we obtain the following structure
%
\begin{align*}
	\dot{x} &= \left[ \begin{array}{cc} 0 & 1  \\  -a_0 & -a_1 \end{array} \right] x 
	+ \left[ \begin{array}{c} 0 \\ 1 \end{array} \right] u
	\\
	y &= \left[ \begin{array}{ccc} ( b_0 - b_2 a_0 ) & ( b_1 - b_2
                                                           a_1 )  \end{array} \right] x
	+ \left[ b_2 \right] u
\end{align*}
%
where 
%
\begin{align*}
a_0= 2 \ , \ a_1 = 3 \ , \ b_0 = 10 \ , \ b_1 = 8 \ ,\& \ b_2 = 1
\end{align*}
%
Thus, the state-space representation takes the form
%
\begin{align*}
	\dot{x} &= \left[ \begin{array}{cc} 0 & 1  \\  -2 & -3 \end{array} \right] x 
	+ \left[ \begin{array}{c} 0 \\ 1 \end{array} \right] u
	\\
	y &= \left[ \begin{array}{ccc} 8 & 5  \end{array} \right] x + [1] u
\end{align*}
%
Observable canonical form is the dual of the controllable canonical form
thus for the given system, we know that
%
\begin{align*}
	A_{OCF} &= A_{CCF}^T = \left[ \begin{array}{cc} 0 & -2  \\  1 & -3 \end{array} \right]
		\\
	B_{OCF} &= C_{CCF}^T = \left[ \begin{array}{c} 8  \\  5 \end{array} \right]
		\\
	C_{OCF} &= B_{CCF}^T = \left[ \begin{array}{cc} 0  &  1 \end{array} \right]
	\\
	 D_{OCF} &= D_{CCF} = [1]
\end{align*}
%
In order to find the diagonal canonical form, we need to perform partial fraction
expansion 
%
\begin{align*}
G(s) &= \frac{s^2 + 8 s + 10}{s^2 + 3 s + 2}
	= 1 + \frac{3}{s+1} + \frac{2}{s+2}
\end{align*}
%
then SS matrices for the diagonal canonical form can be simply derived as
%
\begin{align*}
	A_{DCF} &=  \left[ \begin{array}{cc} -1 & 0  \\  0 & -2 \end{array} \right]
		\\
	B_{DCF} &=  \left[ \begin{array}{c} 1  \\  1 \end{array} \right]
		\\
	C_{DCF} &= \left[ \begin{array}{cc} 3  &  2 \end{array} \right]
	\\
	 D_{DCF} &=  [1]
\end{align*}

\newpage

\textbf{Example:} Consider the following general
state-space representation
%
\begin{align*}
  \dot{x}(t) &= A x(t) + B u(t) , \\
  y(t) &= C x(t) + D u(t) 
\end{align*}
%
Now let's consider the following
state-space representation
%
\begin{align*}
  \dot{\bar{x}}(t) &= A^T \bar{x}(t) + C^T u(t) , \\
  y(t) &= B^T \bar{x}(t) + D u(t) 
\end{align*}
%
Show that these two state-space representations 
results in same transfer function form

\vspace{6pt}

\textbf{Solution:} 
%
For the second representation we have 
%
\begin{align*}
\bar{G}(s) &= \bar{C} \left( s I - \bar{A} \right)^{-1} \bar{B} + D
	\\
	&= B^T \left( s I - A^T \right)^{-1} C^T + D
\end{align*}
%
Since $\bar{G}(s)$ is a scalar quantity we can take its
transpose 
%
\begin{align*}
	\bar{G}(s) &= [\bar{G}(s)]^T = 
	[ B^T \left( s I - A^T \right)^{-1} C^T + D ]^T
	\\
	&= (C^T)^T \left( \left( s I - A^T \right)^{-1} \right)^T (B^T)^T + D
	\\
	&= C \left( \left( s I - A^T \right)^T \right)^{-1} B + D
	\\
	&= C \left( s I - A \right)^{-1} B + D
	\\
	\bar{G}(s) &= G(s)
\end{align*}
%
This result also shows that controllable and observable 
canonical representations are similar. 

\newpage

\section{Stability \& State-Space Representations}

Let's consider the state-representation of an LTI system 
%
\begin{align*}
  \dot{x}(t) &= A x(t) + B u(t)
\\
  y(t) &= C x(t) + D u(t)
\end{align*}
%
Given LTI system is called \textit{asymptotically stable} if,
with $u(t) = 0$ and $\forall x(0) \in \mathbb{R}^n$, we have
%
\begin{align*}
  \lim_{t \to \infty} || x(t) || = 0
\end{align*}

\textit{Theorem:} A state-space representation is 
asymptotically stable if and only if all of the eigenvalues
of the system matrix, $A$, have negative real parts, i.e.
%
\begin{align*}
    \forall x_0 \in \mathbb{R}^n, \lim_{t \to \infty} || x(t) || = 0
    \iff \forall i \in \lbrace 1 , \ \cdots ,  \ n \rbrace, \mathrm{Re} \lbrace \lambda_i  \rbrace < 0
\end{align*}
%

\vspace{6pt}

\textbf{Ex:} Show that if a state-space representation is asymptotically stable
then its transfer function representation is BIBO stable.

\vspace{6pt}

\textbf{Solution:} Previously we showed that 
if  $p$ is a pole of $G(s)$, then it is also an
eigenvalue of $A$, since we can write $G(s)$ as 
%
\begin{align*}
 G(s) &= \frac{C \mathrm{Adj} \left(s I - G \right) B + D \mathrm{det}
        \left(s I - A
                               \right)}{\mathrm{det} \left(s I - A
                               \right) }
\end{align*}
%
If the state-space representation is asymptotically stable then
we know that for each pole, $p_i$ of $G(s)$ we have $\mathrm{Re} \lbrace p_i  \rbrace < 0$
which makes the input--output dynamics BIBO stable. In conclusion,

Asymptoticly stable $\Rightarrow$ BIBO stable
%

\textbf{Example:} Consider the following state-space form of a
CT system 
%
\begin{align*}
\dot{x}(t) &= 
\left[ \begin{array}{cc} 0 & 1 \\ 1 & 0 \end{array} \right]
x(t)
+
\left[ \begin{array}{c} 0 \\ 1 \end{array} \right] u(t)
\\
y(t) &= \left[ \begin{array}{cc} 1 & -1 \end{array} \right] x(t)
\end{align*}

\begin{itemize}
	\item Is this system asymptotically stable? 
	\item Is this system BIBO stable?
\end{itemize}

\textbf{Solution:} Let's compute the eigenvalues of $A$

\begin{align*}
\mathrm{det} \left( \left[ \begin{array}{cc} \lambda & -1 \\ -1 &
                                                                  \lambda \end{array}
                                                                  \right]
                                                                  \right)
                                                                  =
                                                                  \lambda^2
                                                                  - 1
\\
\lambda_{1,2} = \pm 1
\end{align*}

Thus the system is NOT Asymptotically Stable.
Now let's check  BIBO stability condition. 
First, compute the $G(s)$

\begin{align*}
  G(s) &= \left[ \begin{array}{cc} 1 & -1 \end{array} \right]
\left[ \begin{array}{cc} s & -1 \\ -1 & s\end{array}
                                                                  \right]^{-1}
\left[ \begin{array}{c} 0 \\ 1 \end{array} \right]
\\
&=
\left[ \begin{array}{cc} 1 & -1 \end{array} \right]
\left[ \begin{array}{cc} s & 1 \\ 1 & s\end{array}
                                                                  \right]
\left[ \begin{array}{c} 0 \\ 1 \end{array} \right] \frac{1}{s^2 - 1}
\\
&=
\left[ \begin{array}{cc} 1 & -1 \end{array} \right]
\left[ \begin{array}{cc} s & 1 \\ 1 & s\end{array}
                                                                  \right]
\left[ \begin{array}{c} 0 \\ 1 \end{array} \right] \frac{1}{s^2 - 1}
\\
&=
\left[ \begin{array}{cc} 1 & -1 \end{array} \right]
\left[ \begin{array}{c} 1 \\ s \end{array} \right] \frac{1}{s^2 - 1}
\\
&= \frac{-(s-1)}{s^2 - 1} 
\\
&= \frac{-1}{s + 1} 
\end{align*}

Indeed, the system is BIBO Stable.

In conclusion

\begin{itemize}
  \item Asymptotically Stable $\Rightarrow$ BIBO Stable
  \item BIBO Stable $\not\Rightarrow$ Asymptotically Stable
\end{itemize} 


\end{document}
